<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Shell: list, download and extract S3 log files </title>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" type="text/css" href="../assets/css/styles.css">
</head>
<body>
<div class="container">
<a href="../index.html" class="back-link">&laquo; Back to Index</a>
<div class="markdown-content">
<h1 id="shell-list-download-and-extract-s3-log-files">Shell: list, download and extract S3 log files</h1>

<p><a href="https://gist.github.com/Integralist/8b58308965b9b1839f3c76eb610d43ab" target="_blank">View original Gist on GitHub</a></p>

<p><strong>Tags:</strong> #aws #logs #shell</p>

<h2 id="list-download-and-extract-s3-log-files-sh">list, download and extract S3 log files.sh</h2>

<pre><code class="language-shell">#!/usr/bin/env bash

# WARNING: this can exhaust process resources
# there is no process pool implementation

function get_logs {
  aws s3 ls &quot;bf-logs-archive/Fastly/$1/&quot; &gt; ls_results.txt
}

function copy_log {
  aws s3 cp &quot;s3://bf-logs-archive/Fastly/$1/$2&quot; ./logs/ &amp;
}

function process {
  local dir=$1
  local logs=$2
  local file

  while read -r line; do
    file=$(echo &quot;$line&quot; | cut -d ' ' -f 10)

    if [[ $file != &quot;&quot; ]]; then
      copy_log &quot;$dir&quot; &quot;$file&quot;
    fi
  done &lt; &quot;$logs&quot;

  wait
}

function decode_gzipped_files {
  # note: this will replace .gz with the extract log file
  find ./logs -name '*.gz' -exec gzip -d {} \;
}

time get_logs &quot;video-player.buzzfeed.com&quot;
time process &quot;video-player.buzzfeed.com&quot; &quot;ls_results.txt&quot;
time decode_gzipped_files
</code></pre>
</div>
<link rel="stylesheet" href="../assets/css/highlights/github.min.css">
<script src="../assets/js/highlight.min.js"></script>
<script src="../assets/js/notes.js" defer></script>
<script>hljs.highlightAll();</script>

</div>
</body>
</html>