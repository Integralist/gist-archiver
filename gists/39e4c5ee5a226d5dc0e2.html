<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Designing Systems and Applications</title>
<link rel="stylesheet" type="text/css" href="../assets/css/styles.css">
</head>
<body>
<div class="container">
<a href="../index.html" class="back-link">&laquo; Back to Index</a>
<h1 id="designing-systems-and-applications">Designing Systems and Applications</h1>

<p><a href="https://gist.github.com/Integralist/39e4c5ee5a226d5dc0e2" target="_blank">View original Gist on GitHub</a></p>

<h2 id="1-designing-systems-and-applications-md">1. Designing Systems and Applications.md</h2>

<h1 id="designing-systems-and-applications-1">Designing Systems and Applications</h1>

<p>This is a short document of tips and notes I&rsquo;ve accumulated while learning more about designing distributed systems and building concurrent applications. It is by no means definitive and merely scratches the surface of what is needed to be considered when designing an architecture expected to handle large scale traffic.</p>

<h2 id="distributed-systems">Distributed Systems</h2>

<h3 id="scale-out-not-up">Scale <em>out</em>, not <em>up</em></h3>

<p>There reaches a point in your application&rsquo;s design where by merely throwing more hardware at the problem (i.e. &ldquo;scaling up&rdquo;) will fail to resolve the scalability issues you&rsquo;re encountering.</p>

<p>You should aim for an system designed to scale horizontally (i.e. &ldquo;scaling out&rdquo;) as it allows for easier growth and improvement.</p>

<h3 id="build-smaller-applications">Build smaller applications</h3>

<p>If you currently have a monolithic application, then you should consider drawing a top level diagram of your current architecture. Find logical areas of the application which can be split into separate services that communicate with each other. This allows each individual service to scale separately depending on its load.</p>

<blockquote>
<p>Note: mechanisms for communication could be pubsub (e.g. AWS SNS)<br />
or via a push/pull design using queues (e.g. AWS SQS)<br />
and use temporary storage to pass between services (e.g. AWS S3)</p>
</blockquote>

<p>The discussion of designing modular systems and applications (which are connected via different mechanisms to potentially avoid single points of failure) will always cause some contention between the arguments of &ldquo;pure scalability&rdquo; &amp; &ldquo;economical financial stability&rdquo;; e.g. smaller set of servers sharing multiple services is cheaper than a individual servers for each service. The outcome will depend on the size of the organisation and whether it&rsquo;s financially viable to support an extremely granular architecture.</p>

<h3 id="measure-monitor-log">Measure, Monitor, Log</h3>

<p>Implement solutions to application logging and server monitoring as soon as possible. Aim to <em>prevent</em> disasters rather than just automating recovery from them. Identify unhealthy services before they become an unrecoverable problem.</p>

<p>Also, by measuring and monitoring appropriately, we can ascertain whether our instances are too small/large and can be scaled up/down accordingly.</p>

<h3 id="utilization">Utilization</h3>

<p>Don&rsquo;t just think about automatically scaling; you should definitely scale where appropriate, but also think about optimizing utilization. For example, look at your application/service design and look at ways to modify them so they can handle more traffic, more efficiently. Doing this will help to reduce costs by not needing to scale as much as we&rsquo;ll be better utilizing the resources.</p>

<h3 id="single-points-of-failure-spof">Single Points of Failure (SPOF)</h3>

<p>You should look to identify weak spots (e.g. &ldquo;single points of failure&rdquo;) in your architecture. These are places where by if any part of your application or service goes down, then the whole application becomes useless (see next section on failing gracefully). See the <a href="https://gist.github.com/Integralist/39e4c5ee5a226d5dc0e2#file-designing-systems-md" target="_blank">architecture diagrams at the bottom of this page</a> which demonstrate a simple application which had a SPOF but re-architectured the applications and services so they could better handle potential failures and keep users unaware there was a problem with the system.</p>

<h3 id="fail-gracefully">Fail Gracefully</h3>

<p>Certain components can&rsquo;t scale easily (such as databases &amp; nosql document stores). In an ideal world we would build the application to fail gracefully. For example, we could monitor/watch for &ldquo;warning&rdquo; thresholds; and when critical mass is reached and the relevant alarm is fired (let&rsquo;s say an AWS SNS notification is sent) our applications (if subscribed to the SNS topic) would be able to take action to store off important data in an external service (in case of imminent failure) or worst case temporarily disabling certain features so the user doesn&rsquo;t lose important data.</p>

<p>How we fail should be determined by us and our application and will be dependent on the application being built (i.e. you couldn&rsquo;t fail in the exact same way for every application type as the requirements would be vastly different).</p>

<h3 id="caching">Caching</h3>

<p>Analyse your architecture not only for SPOF but also to see where we can implement caching layers to reduce traffic load.</p>

<h2 id="concurrency">Concurrency</h2>

<p>Concurrency introduces many different types of problems. As an example, think of the classic &ldquo;Banker&rsquo;s Dilemma&rdquo; where by you have two customers: John and Bob who both have access to a joint account, which currently holds £10. If both John and Bob trigger an action at the same time (let&rsquo;s say John takes out £10 and Bob takes out £5) then what should be the outcome? If John&rsquo;s action was handled first then the account balance should be 0, but if Bob&rsquo;s action was triggered at the same time but didn&rsquo;t finish as quickly then we have an issue where he&rsquo;ll be attempting to take £5 from a 0 balance account.</p>

<h3 id="thread-safety">Thread Safety</h3>

<p>Although a much larger discussion by itself, the principle of data being &ldquo;thread safe&rdquo; is that it is accessible via multiple threads at the same time and will not cause conflict. Usually have globally shared state can help, or better yet, writing applications that work with the idea of immutable data (where by a copy of the modified data is used and the original is left untouched - see languages such as Haskell and Clojure).</p>

<blockquote>
<p>Note: for more information on thread safety, have a read of<br />
<a href="http://en.wikipedia.org/wiki/Thread_safety" target="_blank">http://en.wikipedia.org/wiki/Thread_safety</a></p>
</blockquote>

<h3 id="eventual-consistency">Eventual Consistency</h3>

<p>The principle of &ldquo;eventual consistency&rdquo; is implemented in distributed systems as a way to ensure that a data change will &ldquo;eventually&rdquo; be applied to all available nodes (i.e. it wont be immediate; as that is the tradeoff with the principle of &ldquo;high availability&rdquo;).</p>

<p>If you want a change to be made immediately across multiple nodes then those nodes would need to be locked down long enough for the change to be replicated throughout. This could be a lengthy process and so the eventual consistency model was designed to keep availability high and allow for systems to keep running until the change had been safely applied throughout all available nodes.</p>

<h3 id="cpu-vs-i-o">CPU vs I/O</h3>

<p>A CPU/Processor can contain one or more cores. For example, a quad core processor that runs at speed of 3GHz will have 4 cores running at that speed.</p>

<p>I/O, whether a file system action or Network - e.g. HTTP - action, can block and so if the application is designed to work concurrently (e.g. there are other threads the CPU can jump to in the mean time) then the current thread will be left to finish and another thread will be picked up instead (this is how concurrency works - the CPU interleaves between threads - this should also clarify how concurrency <em>is not</em> the same thing as paralleism).</p>

<p>For computational intensive operations you&rsquo;ll want the number of threads to be equal to the number of cores available.</p>

<p>For I/O intensive operations you&rsquo;ll want more threads than available cores. This is because (as explained above) the CPU/Processor will &ldquo;context switch&rdquo; to another thread when the current thread is blocked (hence it is better to have more threads than cores for I/O).</p>

<h3 id="calculating-the-number-of-threads">Calculating The Number of Threads</h3>

<p>To calculate how many more threads than cores you&rsquo;ll need for an intensive set of I/O operations, use the following algorithm:</p>

<p>Number of Threads = Number of Available Cores / (1 - Blocking Coefficient)</p>

<blockquote>
<p>Note: the blocking coefficient (coefficient being a fancy word that means: a value used as a multiplier) is different depending on the operation. For a computational operation it is 0, where as a fully blocking operation it is 1.</p>
</blockquote>

<p>An example of a blocking coefficient would be: <code>0.9</code> - which means a task blocks 90% (<code>0.9</code>) of the time &amp; works only 10% (<code>0.1</code>) of the time. Meaning, if you has 2 cores then you&rsquo;d want 20 threads.</p>

<p><code>2 / (1 - 0.9) = 20</code></p>

<h3 id="thread-pools">Thread Pools</h3>

<p>A thread pool is a collection of pre-determined threads that automatically handles the management of tasks from a queue. Thread pools can sometimes be more efficient (and practical) than manually maintaining individual threads via your own application. Languages such as Java (and indirectly JRuby) has built-in support for thread pools.</p>

<p><img src="https://cloud.githubusercontent.com/assets/180050/4523399/f7b9065c-4d36-11e4-98ed-65eb7b80313e.png" alt="thread pool" /></p>

<h3 id="even-workload-distribution">Even Workload Distribution</h3>

<p>If you have two cores and a very large queue of messages to process, then your initial thought would maybe be to split the queue (i.e. the tasks) into two. This would mean you could have two threads running (i.e. utilising both cores); the first thread processing the first queue data and the second thread handling the other half of the queue data.</p>

<p>The problem with this solution is that is doesn&rsquo;t necessarily guarantee even distribution of the tasks across your available cores. If our queue data consisted of a computational task such as calculating prime numbers then the first half of the queue would take a lot less time to process because the smaller prime numbers would take less time to calculate than the other queue (which if evenly split in two would mean the other queue would have the much larger prime numbers to calculate).</p>

<p>This means one core will be sitting idle while the other core is still processing data.</p>

<p>What would be better is to have more <em>parts</em> than threads/cores. So if one &ldquo;part&rdquo; finishes more quickly than expected, then another part can be picked up. Simply diving our tasks into two parts means one core will likely be sitting idle for longer than the other core. But if we divide our tasks into more granular parts, then we can aim to utilise as much of each core as possible.</p>

<h2 id="aws">AWS</h2>

<h3 id="understanding-autoscaling">Understanding AutoScaling</h3>

<p>ASG contains instances and so requires a Launch Configuration</p>

<p>Launch Configurations determine which instance is launched (AMI &amp; Instance Type)</p>

<p>Alarms are from CloudWatch and they determine when a scaling action (i.e. policy) should take place</p>

<p>Policies specify that instances should be launched or terminated</p>

<p>Scale up/down amount should be a multiple of the number availability zones (so ELB can evenly distribute)</p>

<p>Steps: create launch config (specify AMI &amp; instance type); create ASG (pass in launch config, availability zones, main/max instances, load balancers); create policies (to represent scaling actions); create alarms to trigger policies</p>

<h3 id="calculating-costs">Calculating Costs</h3>

<p>Use the official <a href="http://calculator.s3.amazonaws.com/index.html" target="_blank">AWS Simple Monthly Calculator</a> to estimate costs of all services used. This is to help communicate to the business that we&rsquo;re aware of cost concerns and are designing our applications to be as cost effective as possible.</p>

<h3 id="prevention-and-reaction">Prevention <em>AND</em> Reaction</h3>

<p>As mentioned earlier, we don&rsquo;t want to just react to issues, but be prepared by proactively monitoring our services and utilizing resources such as CloudWatch alarms as a way to indicate preventative thresholds be crossed.</p>

<p>We should be looking for sustained high usage and not solely worrying about specific spikes/peaks in traffic. High usage could be an indicator of a potential optimisation hot spot.</p>

<p>We should also be considerate of high profile events in our industry. With this knowledge we could indicate a potential to pre-warm our servers rather than just reacting to sudden high traffic loads.</p>

<h3 id="analyse-groups">Analyse Groups</h3>

<p>We should try to utilise CloudWatch within the AWS console to analyse multiple servers at once. In doing this we can then monitor AutoScaling groups to ensure the group, as a whole, is healthy - rather than simply analysing smaller instances and not getting a view of the bigger picture.</p>

<p>Depending on the system design and workflow of your application, the throughput on a queue should give us a correlating throughput in subsequent queues. For example, you might have a queue that takes in messages for orders to be processed, then once processed the successful order details are sent to a subsequent queue which are processed and emailed to the relevant customers. The &ldquo;processing&rdquo; queue &amp; &ldquo;successful orders&rdquo; queue should have similar throughput - giving us a useful indication of a healthy overall system (i.e. we&rsquo;re not just processing orders and failing to process the emails to customers).</p>

<h2 id="designing-systems-md">Designing Systems.md</h2>

<p>Images taken from <a href="http://www.slideshare.net/IanMassingham/aws-devops-event-aws-services-enabling-devops-automated-testing-monitoring-and-continuous-deployment" target="_blank">http://www.slideshare.net/IanMassingham/aws-devops-event-aws-services-enabling-devops-automated-testing-monitoring-and-continuous-deployment</a></p>

<h2 id="bad-application-design">Bad Application Design</h2>

<p><img src="https://cloud.githubusercontent.com/assets/180050/4507524/4db2f3e6-4b0d-11e4-9f73-9d446e23a838.jpg" alt="1" />
<img src="https://cloud.githubusercontent.com/assets/180050/4507527/4db9cc52-4b0d-11e4-9e0d-6af792cd0b1c.jpg" alt="2" /></p>

<h2 id="good-application-design">Good Application Design</h2>

<p><img src="https://cloud.githubusercontent.com/assets/180050/4507529/4dc06d50-4b0d-11e4-8b94-181b147d242c.jpg" alt="3" />
<img src="https://cloud.githubusercontent.com/assets/180050/4507528/4dbdf11a-4b0d-11e4-8f0a-a01f057cad3e.jpg" alt="4" />
<img src="https://cloud.githubusercontent.com/assets/180050/4507525/4db828de-4b0d-11e4-8844-c0dbc475495c.jpg" alt="5" />
<img src="https://cloud.githubusercontent.com/assets/180050/4507526/4db8b6e6-4b0d-11e4-85c6-1633e9e065ff.jpg" alt="6" />
<img src="https://cloud.githubusercontent.com/assets/180050/4507530/4dc33f6c-4b0d-11e4-89ed-3a2edf53bf41.jpg" alt="7" />
<img src="https://cloud.githubusercontent.com/assets/180050/4507531/4dca75ac-4b0d-11e4-8b45-3fb232fa909b.jpg" alt="8" />
<img src="https://cloud.githubusercontent.com/assets/180050/4507532/4dccd464-4b0d-11e4-9602-827647debd7a.jpg" alt="9" /></p>

</div>
</body>
</html>