<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Demonstrating the set-up of CoreOS and how to utilise Etcd along with Systemd and Fleet</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><link rel="stylesheet" type="text/css" href="../assets/css/styles.css">
</head>
<body>
<div class="container">
<a href="../index.html" class="back-link">&laquo; Back to Index</a>
<div class="markdown-content">
<h1 id="demonstrating-the-set-up-of-coreos-and-how-to-utilise-etcd-along-with-systemd-and-fleet">Demonstrating the set-up of CoreOS and how to utilise Etcd along with Systemd and Fleet</h1>

<p><a href="https://gist.github.com/Integralist/43574477639b327e1d8a" target="_blank">View original Gist on GitHub</a></p>

<h2 id="coreos-essentials-md">CoreOS Essentials.md</h2>

<h2 id="getting-started">Getting started</h2>

<ul>
<li><code>git clone git@github.com:coreos/coreos-vagrant.git</code></li>
<li><code>mv user-data.sample user-data</code></li>
<li>Change <code>etcd2</code> to have the following content:</li>
</ul>

<pre><code class="language-ini">advertise-client-urls: http://$public_ipv4:2379,http://$public_ipv4:4001
initial-advertise-peer-urls: http://$private_ipv4:2380
listen-client-urls: http://0.0.0.0:2379,http://0.0.0.0:4001
listen-peer-urls: http://$private_ipv4:2380,http://$private_ipv4:7001
initial-cluster-token: core-01_etcd
initial-cluster: core-01=http://$private_ipv4:2380
initial-cluster-state: new
</code></pre>

<h2 id="services">Services</h2>

<ul>
<li>Systemd: CoreOS&rsquo;s init system</li>
<li>Etcd: distributed key-value store</li>
<li>Fleet: distributed init system (ties together systemd and etcd; NOT a container orchestrator)</li>
</ul>

<h2 id="setup">Setup</h2>

<pre><code class="language-bash"># Access the CoreOS VM
vagrant up
vagrant ssh

# Check etcd and fleet are running
systemctl status etcd2
systemctl status fleet

# Check installed version of Docker
docker version
</code></pre>

<h2 id="etcd">Etcd</h2>

<pre><code class="language-bash"># Verify Host/VM access to etcd
etcdctl set /foo bar
etcdctl get /foo
etcdctl rm  /foo
etcdctl set /foo-dir/foo-key &quot;foo value&quot;
etcdctl set /foo-dir/bar-key &quot;bar value&quot;
etcdctl ls  /foo-dir --recursive # works without --recursive and also with -recursive ?
etcdctl get /foo-dir/foo-key
</code></pre>

<h2 id="http-api">HTTP API</h2>

<blockquote>
<p>Etcd has an HTTP API<br />
The API always responds with JSON<br />
e.g. <code>{&quot;action&quot;:&quot;get&quot;,&quot;node&quot;:{&quot;key&quot;:&quot;/baz&quot;,&quot;value&quot;:&quot;qux&quot;,&quot;modifiedIndex&quot;:884,&quot;createdIndex&quot;:884}}</code><br />
It is useful for client apps and host containers that don&rsquo;t have etcdctl installed</p>
</blockquote>

<pre><code class="language-bash"># Verify HTTP API access to etcd
curl --location --request PUT http://127.0.0.1:2379/v2/keys/baz --data value=&quot;qux&quot;
curl --location http://127.0.0.1:2379/v2/keys/baz
curl --location --request DELETE http://127.0.0.1:2379/v2/keys/baz

# Check docker0 interface IP
echo &quot;$(ifconfig docker0 | awk '/\&lt;inet\&gt;/ { print $2 }'):2379&quot;

# Run a container from the Alpine Linux distro and access etcd from within it
docker run -it alpine ash # that's not a typo &quot;ash&quot; is correct
apk update &amp;&amp; apk add curl
curl --location http://&lt;docker0 interface IP&gt;:2379/v2/keys/
</code></pre>

<h2 id="watching">Watching</h2>

<pre><code class="language-bash"># Watch etcd for changes (requires two terminal shells)
etcdctl mkdir /foo-data
etcdctl watch /foo-data --recursive # unlike `ls` you MUST use --recursive flag (-recursive also works?)
etcdctl watch /foo-data --recursive --forever
</code></pre>

<p>From a second terminal shell&hellip;</p>

<pre><code class="language-bash">vagrant ssh
etcdctl set /foo-data/can-you-see-me-now yes
</code></pre>

<h2 id="ttls">TTLs</h2>

<pre><code class="language-bash"># Verify etcd's TTL feature
etcdctl set /my-ttl &quot;I'm expiring in 30 seconds...&quot; --ttl 30
etcdctl get /my-ttl
</code></pre>

<p>&hellip;30 seconds later&hellip;</p>

<pre><code class="language-bash">etcdctl get /my-ttl
</code></pre>

<h2 id="systemd">SystemD</h2>

<blockquote>
<p>Control of &lsquo;unit&rsquo; files/services are on a per machine basis<br />
So only use for simple tasks</p>
</blockquote>

<pre><code class="language-bash"># Verify systemd's unit file setup for managing container processes
sudo vi /etc/systemd/system/hello.service
</code></pre>

<p>Add following content to <code>hello.service</code>:</p>

<pre><code class="language-ini">[Unit]
Description=HelloWorld
# this unit will only start after docker.service
After=docker.service
Requires=docker.service

[Service]
TimeoutStartSec=0
# busybox image will be pulled from docker public registry
ExecStartPre=/usr/bin/docker pull busybox
# we use rm just in case the container with the name &quot;busybox1&quot; is left
ExecStartPre=-/usr/bin/docker rm busybox1
# we start docker container
ExecStart=/usr/bin/docker run --rm --name busybox1 busybox /bin/sh -c &quot;while true; do echo Hello World; sleep 1; done&quot;
# we stop docker container when systemctl stop is used
ExecStop=/usr/bin/docker stop busybox1

[Install]
WantedBy=multi-user.target
</code></pre>

<p>Enable the service:</p>

<blockquote>
<p>This creates a symlink from an internal user directory to the main system directory</p>
</blockquote>

<pre><code class="language-bash">sudo systemctl enable /etc/systemd/system/hello.service
</code></pre>

<p>Start the service:</p>

<blockquote>
<p>It can take a few seconds to complete as it has to pull a docker image from DockerHub</p>
</blockquote>

<pre><code class="language-bash">sudo systemctl start hello.service
</code></pre>

<p>Verify the service:</p>

<pre><code class="language-bash">journalctl -f -u hello.service
</code></pre>

<p>Responds with tailed output:</p>

<pre><code class="language-bash">-- Logs begin at Thu 2015-10-29 08:42:56 UTC. --
Oct 29 09:29:07 core-01 docker[2119]: Hello World
Oct 29 09:29:08 core-01 docker[2119]: Hello World
Oct 29 09:29:09 core-01 docker[2119]: Hello World
Oct 29 09:29:10 core-01 docker[2119]: Hello World
Oct 29 09:29:11 core-01 docker[2119]: Hello World
Oct 29 09:29:12 core-01 docker[2119]: Hello World
Oct 29 09:29:13 core-01 docker[2119]: Hello World
</code></pre>

<blockquote>
<p><code>docker ps</code> will also show the container is running</p>
</blockquote>

<p>Check the status of the service:</p>

<pre><code class="language-bash">sudo systemctl status hello.service
</code></pre>

<p>Stop the service:</p>

<pre><code class="language-bash">sudo systemctl stop hello.service
</code></pre>

<p>Kill the service:</p>

<blockquote>
<p>This doesn&rsquo;t stop the container!<br />
Check <code>docker ps</code> and you&rsquo;ll see it still running<br />
Also <code>journalctl -f -u hello.service</code> will show logs still coming from the container</p>
</blockquote>

<pre><code class="language-bash">sudo systemctl kill hello.service
</code></pre>

<p>Restart the service:</p>

<pre><code class="language-bash">sudo systemctl daemon-reload # only needed if you've changed the service file
sudo systemctl restart hello.service
</code></pre>

<p>Disable the service:</p>

<pre><code class="language-bash">sudo systemctl disable hello.service
</code></pre>

<h2 id="fleet">Fleet</h2>

<blockquote>
<p>Fleet is a cluster manager that controls Systemd at the cluster level<br />
It can still be used on a single machine</p>
</blockquote>

<p>Fleet unit files are the same as Systemd unit files, but with additional properties that help them identify in the context of a cluster. The only other difference between them is that there is no <code>[Install]</code> in the fleet unit files, as this is replaced with <code>[X-Fleet]</code>.</p>

<p>Let&rsquo;s see what machines are available:</p>

<pre><code class="language-bash">fleetctl list-machines
</code></pre>

<p>Which responds with:</p>

<pre><code>MACHINE     IP            METADATA
46acb6e7... 172.17.8.101  -
</code></pre>

<p>Fleet unit files can be placed any where on the system and then &lsquo;submitted&rsquo; to fleet.</p>

<p>Create the following fleet unit file twice (change the description and url to ping) and name the files <code>ping.1.service</code> and <code>ping.2.service</code>:</p>

<blockquote>
<p>e.g. for the second version you could use<br />
<code>Description=Ping BBC</code><br />
<code>ExecStart=/usr/bin/ping bbc.co.uk</code></p>
</blockquote>

<pre><code class="language-ini">[Unit]
Description=Ping Google

[Service]
ExecStart=/usr/bin/ping google.com

[X-Fleet]
</code></pre>

<blockquote>
<p>We keep <code>X-Fleet</code> empty because with one machine, specifying other settings will cause things to break (I&rsquo;ve yet to figure out why?)</p>
</blockquote>

<p>&ldquo;Conflicts&rdquo; prevents the unit being collocated with the other unit using glob-matching. Where &ldquo;MachineMetadata&rdquo; can be used to determine if an instance in the cluster is acceptable. For example:</p>

<pre><code class="language-ini"># Allow a cluster instance to run this unit 
# Only if they have region set to one of these values
MachineMetadata=region=us-east-1
MachineMetadata=region=us-west-1
</code></pre>

<p>Or alternatively bundle multiple requirements into a single item:</p>

<pre><code class="language-ini">MachineMetadata=&quot;region=us-east-1&quot; &quot;diskType=SSD&quot;
</code></pre>

<p>Submit the fleet unit files:</p>

<pre><code class="language-bash">fleetctl submit ping.1.service
fleetctl submit ping.2.service
</code></pre>

<p>Review list of unit files available:</p>

<pre><code class="language-bash">fleetctl list-unit-files
</code></pre>

<p>Which responds with:</p>

<pre><code>UNIT            HASH    DSTATE    STATE     TARGET
ping.1.service  4764f30 inactive  inactive  -
ping.2.service  683fbdc inactive  inactive  -
</code></pre>

<p>You can remove services from the fleet using <code>destroy</code>:</p>

<pre><code class="language-bash">fleetctl destroy ping.1.service
fleetctl destroy ping.2.service
</code></pre>

<p>To start the fleet services:</p>

<pre><code class="language-bash">fleetctl start ping.1.service
fleetctl start ping.2.service
</code></pre>

<p>Now listing the unit files again:</p>

<pre><code class="language-bash">UNIT            HASH    DSTATE    STATE     TARGET
ping.1.service  4c2ca38 launched  launched  46acb6e7.../172.17.8.101
ping.2.service  86f264e launched  launched  46acb6e7.../172.17.8.101
</code></pre>

<p>You can also verify the services are running using <code>journalctl</code>:</p>

<pre><code class="language-bash">journalctl -f -u ping.1.service
journalctl -f -u ping.2.service
</code></pre>

<h2 id="cloud-config-vs-user-data">Cloud Config vs User Data</h2>

<p>When using Vagrant to set-up a local cluster, you&rsquo;ll have a <code>user-data</code> file, which is really a <code>cloud-config</code> file in the cloud world.</p>

<p>When using Vagrant you&rsquo;ll also have a <code>config.rb</code> which is used to construct the cloud-config file by manipulating the local <code>user-data</code> file.</p>

<h2 id="creating-files-on-all-clusters">Creating files on all clusters</h2>

<pre><code class="language-yaml">write_files:
  - path: /home/core/test.txt
    permissions: 0644
    owner: core
    content: |
      Beep Boop
</code></pre>

<p>Then run the following commands:</p>

<p><code>vagrant provision &amp;&amp; vagrant reload</code></p>

<blockquote>
<p>Note: make sure it&rsquo;s not nested inside the top-level <code>coreos</code> section! as it won&rsquo;t work</p>
</blockquote>

<h2 id="added-metadata-to-all-clusters">Added metadata to all clusters</h2>

<pre><code class="language-yaml">coreos:
  fleet:
    metadata: cluster=vagrant
</code></pre>

<p>Then SSH into the instance (e.g. <code>vagrant ssh core-01</code>) and then run:</p>

<pre><code class="language-bash">fleetctl list-machines
</code></pre>

<p>Which responds with:</p>

<pre><code>MACHINE		  IP		        METADATA
5fcf399b...	172.17.8.101	cluster=vagrant
987c97e7...	172.17.8.102	cluster=vagrant
c15aed24...	172.17.8.103	cluster=vagrant
</code></pre>

<h2 id="schedule-a-new-fleet-service-for-the-cluster">Schedule a new fleet/service for the cluster</h2>

<ul>
<li><code>cd coreos-vagrant-localcluster &amp;&amp; vagrant up</code></li>
<li><code>vagrant ssh core-03</code></li>
<li><code>vi hello-cluster.service</code> (see below)</li>
<li><code>fleetctl start hello-cluster.service</code></li>
<li><code>journalctl -u hello-cluster.service -f</code></li>
</ul>

<h3 id="hello-cluster-service">hello-cluster.service</h3>

<pre><code class="language-ini">[Unit]
[Service]
ExecStart=/usr/bin/bash -c &quot;while true; do echo 'Hello Cluster'; sleep 1; done&quot;
</code></pre>

<h3 id="response-from-fleetctl-command">Response from <code>fleetctl</code> command</h3>

<pre><code>Unit hello-cluster.service inactive
Unit hello-cluster.service launched on 5fcf399b.../172.17.8.101
</code></pre>
</div>
<link rel="stylesheet" href="../assets/css/highlights/github.min.css">
<script src="../assets/js/highlight.min.js"></script>
<script src="../assets/js/notes.js" defer></script>
<script>hljs.highlightAll();</script>

</div>
</body>
</html>