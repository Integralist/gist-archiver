<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>[CDN Logs Sampling Rates] #cdn #logs #sampling #maths #requests #traffic #load</title>
<style>
body { font-family: sans-serif; margin: 20px; line-height: 1.6; background-color: #f4f4f4; color: #333; }
.container { max-width: 800px; margin: auto; background: #fff; padding: 20px; border-radius: 8px; box-shadow: 0 0 10px rgba(0,0,0,0.1); }
pre { background: #f0f0f0; padding: 1em; border-radius: 5px; overflow-x: auto; border: 1px solid #ddd; }
code { font-family: 'SFMono-Regular', Consolas, 'Liberation Mono', Menlo, Courier, monospace; font-size: 0.9em;}
h1, h2 { border-bottom: 1px solid #eee; padding-bottom: 0.3em;}
a.back-link { display: inline-block; margin-bottom: 20px; color: #007bff; text-decoration: none; }
a.back-link:hover { text-decoration: underline; }
</style>
</head>
<body>
<div class="container">
<a href="../index.html" class="back-link">&laquo; Back to Index</a>
<h1 id="cdn-logs-sampling-rates-cdn-logs-sampling-maths-requests-traffic-load">[CDN Logs Sampling Rates] #cdn #logs #sampling #maths #requests #traffic #load</h1>

<h2 id="cdn-logs-sampling-rates-md">CDN Logs Sampling Rates.md</h2>

<pre><code class="language-markdown"># CDN Uncached Request Log Sampling

## Why?

When we began passing Fastly logs through to Datadog, we were concerned with the volume of requests that we'd be logging. To cut down on this volume we decided to only pass through requests that were not satisfied by the cache. While this has reduced the log volume to a great degree, at times of high traffic we've seen millions of log events still passed through. To remain cost conscious while using Datadog to get visibility into these logs a sampling strategy should be implemented.

## Proposed Sampling Rates

| Fastly Status | Meaning of Status | Sample Rate | Justification |
|---|---|---|---|
| `PASS` | serving uncached content and do not intend to cache response | 100% | `PASS` requests are frequently just administrative, however having them easily available is useful for debugging. As there were 66k events for this category total in the sample time period chosen, sending 100% of them to Datadog seems like a reasonable approach. |
| `MISS` | serving uncached content | 10% | `MISS` is the vanilla &quot;uncached content&quot; response and these logs reflect expected behavior. We can sample this category quite conservatively. Rather than relying on logs to expose major changes to this, we should ensure appropriate metrics are taken, and use full logs pulled from S3 when deeper insight is needed. |
| `MISS-WAIT`| serving uncached content and had to wait an unusual amount of time for a highly contested object | 100% | In the sample time period chosen, over 3m events were shipped. 2 of them had the cache response status of `MISS-WAIT` and both cases returned a `400`. These are useful logs to have in completeness for debugging issues with the CDN. |
| `MISS-CLUSTER` | serving uncached content, object will be served from another node in the PoP | 10% | This is the largest category of logs, and for the sample time period chosen, 3.37m log events contained this cache status. Of those, all were successful request fills, with 3.2m of those returning `200`. This is a &quot;healthy&quot; request with an expected behavior and thus can be sampled quite conservatively. Rather than relying on logs to expose major changes to this, we should ensure appropriate metrics are taken, and use full logs pulled from S3 when deeper insight is needed. |
| `MISS-WAIT-CLUSTER` | serving uncached content, object will be served from another node in the pop, waited an unusual amount of time for a highly contested object. | 100% | In the sample time period chosen, over 3m events were shipped. 24 of them had the cache response status of `MISS-CLUSTER-WAIT`. These logs reveal objects that are in high demand and are uncached for whatever reason, which can be a clue in debugging CDN issues and would be good to have easily available.|

## Pricing

Quick napkin math says that adopting this sampling strategy will generate ~ 375k log events/hr, and cost roughly $6.21/day (@ 0.69/1m events) for a total of $187/mo (rounded up).
</code></pre>

</div>
</body>
</html>